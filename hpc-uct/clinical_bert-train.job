#!/bin/sh
#SBATCH --account=nlpgroup --partition=a100
#SBATCH --nodes=1 --ntasks=6
#SBATCH --gres=gpu:ampere:1
#SBATCH --job-name="bert-train"
#SBATCH --mail-user=nrmyas001@cs.uct.ac.za
#SBATCH --mail-type=ALL
#SBATCH --time=08:00:00

module load software/TensorFlow-A100-GPU

export CUDA_VISIBLE_DEVICES=$(ncvd)
export TOKENIZERS_PARALLELISM=false
export WANDB_DIR=/scratch/nrmyas001

cd /home/nrmyas001/clinical-longformer

for i in 1 2 3 4 5
do
      python -m src.clinical_longformer.model.bert  \
          /scratch/nrmyas001/datasets/discharge/512 \
          --do_train \
          --bert_pretrained_path=/scratch/nrmyas001/data/pretraining_output/bert_uncased_L-12_H-768_A-12/512 \
          --default_root_dir=/scratch/nrmyas001/data/model/BERT \
          --attention_probs_dropout_prob=0.2 \
          --hidden_dropout_prob=0.2 \
          --batch_size=8 \
          --accumulate_grad_batches=4 \
          --lr=2e-5 \
          --lr_scheduler_type=linear \
          --warmup_proportion=0.1 \
          --max_epochs=6 \
          --num_workers=6 \
          --stopping_threshold=0.8 \
          --val_check_interval=0.5 \
          --precision=16 \
          --gpus=1 2>&1 | tee /scratch/nrmyas001/logs/bert-train/$(hostname)$(openssl rand -hex 4).out
done
